{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica SPAM Procesamiento del lenguaje natural \n",
    "\n",
    "# Alumno Paul Ivan Trujillo Chavez\n",
    "\n",
    "Los correos electrónicos no deseados en su bandeja de entrada son molestos ya que perturban la rutina del usuarios. Es por eso que las ceuntas de correo electrónico ya tiene un filtro de spam. Dado que e suna de las aplicaciones PLN más utilizadas vamos a ver como se desarrolló un filtro de spam simple para correos electrónicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las famosas librerías\n",
    "from functools import reduce\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insertando los datos\n",
    "full_corpus = pd.read_csv('SMSSpamCollection.tsv', sep='\\t', header=None, names=['label', 'msg_body'])\n",
    "\n",
    "#Separando los mensajes en 'ham' y 'spam'\n",
    "ham_text = []\n",
    "spam_text = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biagrams\n",
    "\n",
    "Los N-gramos se usan para moldear el lenguaje en función de la predicción de palabras, es decir, predice la siguiente palabra de una oración de palabras de una oración de palabras N-1 anteriores. Bigram es la secuencia de 2 palabras de N-gramos que predice la siguiente palabra de una oración usando la palabra anterior. En lugar de considerar la historia completade una oración o una secuencia de palabras en particular, un modelo como bigram puede ser ocupado en términos de una aproximación de la historia al ocupar una historia limitada.\n",
    "\n",
    "La identificación de un mensaje como \"ham\" o \"spam\" es una tarea de clasificación ya que la variable de destino tiene valores discretos que son \"ham\"o \"spam\". En esta práctica, se usa el modelo biagram, aunque existen muchas técnicas avanzadas que se pueden utilizar para este propósito. Para utilizar el modelo bigram para asignar un mensaje dado como \"spam\" o \"ham\", hay varios pasos que deben lograrse.\n",
    "\n",
    "### 1. Inspección y separación de mensajes en las categorías \"ham\" y \"spam\"\n",
    "\n",
    "inicialmente, el conjunto de datos debe inspeccionarse para ocuparlo y abordarlo para lograr la tarea. El formato de los datos dados la cantidad de datos proporcionados, la naturaleza de los datos se incluyen en esta inspección para identificar la mejor aproximación posible para la tarea.\n",
    "\n",
    "El corpus de mensajes dado ha marcado cada mensaje como ham o spam. Además, hay 5568 mensajes en un DataFrame escrito en inglés que no son objetos nulos. Por lo tanto, el archivo tsv se puede leer usando DataFrame en phyton para clasificar esos mensajes de acuerdo con el indicador dado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_msgs():\n",
    "    for index, column in full_corpus.iterrows():\n",
    "        label = column[0]\n",
    "        message_text = column[1]\n",
    "        if label == 'ham':\n",
    "            ham_text.append(message_text)\n",
    "        elif label == 'spam':\n",
    "            spam_text.append(message_text)\n",
    "\n",
    "separate_msgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Procesamiento de texto\n",
    "\n",
    "El procesamiento es la tarea de realizar los pasos de preparación en el corpus de texto sin formato para completar de manera eficiente una extracción de texto o procesamiento de leguaje natural o cualquier otra tarea que incluya texto sin formato. El preprocesaiento de texto consta de varios pasos, aunque algunos de ellos pueden no aplicarse a una tarea en particular debido a la naturaleza del conjunto de datos disponible.\n",
    "\n",
    "En esta tarea, el procesamiento de texto incluye los siguientes pasos de acuerdo con el conjunto de datos:\n",
    "\n",
    "### Eliminación de signos de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminación de los signos de puntuación de los mensajes de correo electrónico\n",
    "def remove_msg_punctuations(email_msg):\n",
    "    puntuation_remove_msg = \"\".join([word for word in email_msg if word not in string.punctuation])\n",
    "    return puntuation_removed_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir a minúsculas:\n",
    "\n",
    "Convertir a minúsculas: La conversión de todos los caracteres del texto en un contexto común, como los soportes en minúsculas, impide identificar dos palabras de madera diferente donde una está en minuscula y la otra no. Por ejemplo, \"primero\" y \"Primero\" deben identificarse como iguales, por lo tanto, poner en minúscula todos los caracteres facilita la tarea. Ademas, las palabras de detención también están en minúsculas, por lo que esto también haría posible eliminar palabras de detención más adelante.\n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "Tokenizing: Es la tarea de dividir el texto en partes significativas, es decir, tokens que incluyen oraciones y palabras. Un token se puede considerar como una instancia de una secuencia de carcateres en un texto particular que se agrupan para proporcionar una unidad semánitca útil para su posterior procesamiento. En esta tarea, la tokenización de palabras se realiza combinando espacios en blanco entre palabras como delimitador. Esto se logra ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convierte el texto en minusculas y tokenizing de palabras\n",
    "def tokenize_into_words(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plabras lematizantes\n",
    "\n",
    "La derivación es el proceso de eliminar (sufijos, prefijos, infijos, circunfijos) de una palabra para obtener su raíz de palabra. Aunque la lematización está relacionada con la derivación, difiere ya que lematización puede capturar formas canónicas basadas en el tema de una palabra. La lematización ocupa un vocabulario y un análisis morfológico de las palabras que los hacan más rápido y preciso que la derivación. WordNetLemmatizer ha logrado la lematización en lenguaje Phyton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatization(tokenized_words):\n",
    "    lemmatized_text = [word_lemmatizer.lemmatize(word)for word in tokenized_words]\n",
    "    return ' '.join(lemmatized_text)\n",
    "\n",
    "def preprocessing_msgs(corpus):\n",
    "    categorized_text = pd.DataFrame(corpus)\n",
    "    categorized_text['non_punc_message_body'] = categorized_text[0].apply(lambda msg: remove_msg_punctuation(msg))\n",
    "    categorized_text['tokenized_msg_body'] = categorized_text['non_punc_message_body'].apply(lambda msg: tokenize_into_words(msg.lower()))\n",
    "    categorized_text['lemmatized_msg_words'] = categorized_text['tokenized_msg_body'].apply(lambda word_list: lemmatization(word_list))\n",
    "    return categorized_text['lemmatized_msg_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extracción de características\n",
    "\n",
    "Después de la etapa  de procesamiento, las caracteristicas deben extraerse del texto. Las caracteristicas son las unidades que admiten la tarea de clasificación, y las bigrams son las caracteristicas en esta tarea de clasificación de mensajes. Los birams o las caracteristicas se extraen dek texto preprocesado Inicialmente, los unigramas se adquieren, y luego esos unigramas se utilizan para obtener los unigramas en cada corpus (\"ham\" y \"spam\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
